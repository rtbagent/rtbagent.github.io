"use strict";(self["webpackChunkrtbagent_vue"]=self["webpackChunkrtbagent_vue"]||[]).push([[335],{335:function(e,t,i){i.r(t),i.d(t,{default:function(){return c}});var a=function(){var e=this;e._self._c;return e._m(0)},n=[function(){var e=this,t=e._self._c;return t("div",{staticClass:"main"},[t("div",{attrs:{id:"title"}},[e._v(" RTBAgent: A LLM-based Agent System for Real-Time Bidding ")]),t("div",{attrs:{id:"title"}},[e._v(" Abstract ")]),t("div",{attrs:{id:"content"}},[e._v(" Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: \\url{https://rtbagent.github.io/}. ")]),t("div",{attrs:{id:"title"}},[e._v(" Overall Framework ")]),t("div",{attrs:{id:"figure"}},[t("img",{staticStyle:{width:"1000px"},attrs:{src:i(6668)}})]),t("div",{attrs:{id:"title"}},[e._v(" Detail Workflow ")]),t("div",{attrs:{id:"figure"}},[t("img",{staticStyle:{width:"1000px"},attrs:{src:i(7926)}})]),t("div",{attrs:{id:"title"}},[e._v(" Discussion ")]),t("div",{staticClass:"advantage"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"500px"},attrs:{src:i(6010)}})]),t("div",{staticClass:"content"},[e._v(" A key challenge within RTB is the development of effective bidding strategies for advertisers. An optimal bidding strategy should promote products to targeted users without disrupting their experience and enhance revenue for publishers. As illustrated in Table 1, traditional rule-based bidding strategies are often too rigid and need to adapt to the dynamic nature of the market. While reinforcement learning (RL) approaches offer better adaptability, they face issues such as the need for extensive training data, difficulties in achieving training convergence, and a lack of interpretability in decision-making, which affects their applicability, trustworthiness, and stability. Consequently, there is a need for more advanced machine-learning models in the RTB domain. we introduce RTBAgent, a novel agent framework designed to address the challenges of competitive advertising bidding environments. ")])]),t("div",{attrs:{id:"title"}},[e._v(" Experiment ")]),t("div",{staticClass:"experiment"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"500px"},attrs:{src:i(7337)}})]),t("div",{staticClass:"content"},[e._v(" In our comparative analysis of performance on the iPinYou dataset under various budget scenarios, RTBAgent consistently outperformed conventional bidding methods and RL approaches, as shown in Table 2. RTBAgent significantly improved the key metric of click number compared to traditional methods such as MCPC, LIN, LP, and ORTB. These traditional methods underperformed in budget management, primarily due to their lack of flexibility in adapting to market volatility, often resulting in early budget depletion. In contrast, RTBAgent’s advanced multi-memory retrieval system and two-step decision-making process allow for dynamic adjustments in bidding strategies, optimizing budget usage and enhancing advertising campaign effectiveness. When compared with RL models like RLB, DRLB and USCB, RTBAgent demonstrated superior adaptability and stability. Although model-based RL methods are generally well-suited to dynamic environments, they showed inconsistent performance under varying budget constraints, suggesting challenges in model training and strategic refinement. ")])]),t("div",{staticClass:"experiment"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"500px"},attrs:{src:i(1616)}})]),t("div",{staticClass:"content"},[e._v(" In Table 3, we introduce three models, MCPC+, LIN+, and LP+, to demonstrate the performance enhancement achieved by RTBAgent with three expert strategies. Specifically, MCPC+, LIN+, and LP+ correspond to RTBAgent based on three different expert strategies, respectively: MCPC, LIN, and LP. Regardless of which method is used, the performance of RTBAgent on the test set consistently improved based on the original performance. It is observed that the stronger the expert strategy performance provided, the better the final results. ")])]),t("div",{staticClass:"experiment4"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"450px"},attrs:{src:i(5138)}})]),t("div",{staticClass:"content"},[e._v(" The performance of RTBAgent, utilizing distinct scale base models for real-time bidding tasks, is examined in Table 4. Comparative analysis reveals that RTBAgent consistently surpasses alternative strategies across all budget allocation ratios. This indicates RTBAgent's superior performance regardless of model scale, from larger, more capable models to smaller, more efficient ones. The agent demonstrates adaptability and scalability, capable of dynamically adapting to the model size in response to business requirements and resource limitations, thus achieving the best bidding outcomes. ")])]),t("div",{staticClass:"experiment"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"500px"},attrs:{src:i(4860)}})]),t("div",{staticClass:"content"},[e._v(" In Table 5, we analyze the effectiveness of expert strategies and two-step decision-making (including insight reasoning and action making). Specifically, the action directly utilizes output optimal scaling factor by LLMs for situations without expert strategy. By comparing rows one and two of Table 5, we can find that incorporating expert knowledge boosts model performance by utilizing historical market information. Furthermore, row three highlights the superiority of a two-step decision-making model over direct action approaches regarding click count increases, emphasizing the value of detailed market analysis and flexible bidding strategies. ")])]),t("div",{attrs:{id:"title"}},[e._v(" Visualization ")]),t("div",{staticClass:"experiment"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"1000px"},attrs:{src:i(7212)}})]),t("div",{staticClass:"content"},[e._v(" Figure 2 demonstrates how RTBAgent utilizes LLMs to enhance its reasoning process during real-time bidding, specifically for advertiser 2997 on the last step of October 27th, 2013. The figure illustrates the core workflow, starting with information gathering, where RTBAgent analyzes historical bidding data, identifying that a reasonable increase in the bidding factor has effectively improved visibility and click volume. This data analysis is the foundation for the next stage, a two-step decision-making process. In the first step, RTBAgent performs insight reasoning by analyzing current market conditions and historical data to extract strategic insights, such as recommending a 0.15 adjustment to the bid factor based on past performance. In the second step, RTBAgent translates these insights into actionable decisions by adjusting the bid price, aiming to optimize performance while maintaining stability. After each bidding cycle, RTBAgent engages in daily reflection, where it evaluates the outcomes of its decisions, compares expected results with actual performance, and updates its strategy accordingly. This iterative process allows RTBAgent to continuously improve its bidding approach, adapting to market dynamics and making more informed, strategic decisions in future cycles. The visualization in Figure \\ref{fig:example}, along with the data presented in Table \\ref{tab:com2step}, underscores RTBAgent's capability to refine the decision-making process in RTB scenarios. The detailed analysis of a single hour for advertiser 2997 on October 27th, 2013, demonstrates RTBAgent’s ability to learn from daily operations and continuously improve its bidding strategies. ")])]),t("div",{staticClass:"experiment"},[t("div",{staticClass:"figure",attrs:{align:"center"}},[t("img",{staticStyle:{width:"700px"},attrs:{src:i(5994)}})]),t("div",{staticClass:"content"},[e._v(" As shown in Figure 3, LP consumed too much budget in the early stages and did not anticipate the benefits of spending the budget in the later stages. On the contrary, LP+ can better control the expenditure of the budget, allowing for the purchase of high-quality clicks with a surplus budget in the second half of the process. In addition, LP+'s CPC has always been lower than the expert strategy throughout the process to ensure that more clicks are obtained within the specified budget. This suggests that RTBAgent effectively utilizes the guidance knowledge of expert strategies, leading to enhanced performance through insight into the environment and interaction. ")])])])}],s={name:"Begin",data(){return{items:[],table1:[{Method:1,Scheme:1,Tools:1,Adaptability:1,Explainability:1}]}},methods:{}},r=s,o=i(1001),d=(0,o.Z)(r,a,n,!1,null,"25dc57dd",null),c=d.exports},6668:function(e,t,i){e.exports=i.p+"img/fig1.06111454.png"},7212:function(e,t,i){e.exports=i.p+"img/fig2.8ccb1bad.png"},5994:function(e,t,i){e.exports=i.p+"img/fig3.bc9d9c77.png"},7926:function(e,t,i){e.exports=i.p+"img/fig4.09a83d11.png"},6010:function(e,t,i){e.exports=i.p+"img/tab1.d43d2224.png"},7337:function(e,t,i){e.exports=i.p+"img/tab2.b44df043.png"},1616:function(e,t,i){e.exports=i.p+"img/tab3.7cdd4f1e.png"},5138:function(e,t,i){e.exports=i.p+"img/tab4.acf85c98.png"},4860:function(e,t,i){e.exports=i.p+"img/tab5.11065a1a.png"}}]);
//# sourceMappingURL=335.93c65faa.js.map